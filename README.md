# Overview

This project extends a Custom LangChain Chat Model to integrate with BAML, creating a bridge between LangChain's powerful LLM orchestration capabilities and BAML's structured data extraction and function calling features.

## ðŸš€ BAML Model Integration TODO

### Core Implementation âœ… 90% Complete
- [x] Create `BamlChatModel` extending `BaseChatModel` âœ…
- [x] Implement `_chat_completion_request()` for BAML API calls âœ… COMPLETED
- [x] Implement `_agenerate()` method (async) âœ… COMPLETED AND TESTED
- [x] Implement `_astream()` method (async streaming) âœ… COMPLETED AND TESTED
- [x] Implement `bind_tools()` method âœ… COMPLETED AND TESTED
- [x] Implement `_convert_to_baml_messages()` method âœ… COMPLETED
- [x] Implement `_prepare_tb()` method âœ… COMPLETED
- [x] Implement `_convert_to_ai_message()` method âœ… COMPLETED
- [ ] Implement `_generate()` method (sync fallback) - **Not implemented by design (async-only architecture)**
- [ ] Implement `_prepare_messages()` for message conversion - **Not needed (replaced by `_convert_to_baml_messages`)**

### Tool Integration âœ… 100% Complete
- [x] Implement `convert_to_baml_tool()` â€” converts Pydantic BaseModel and LangChain `@tool(parse_docstring=True)` functions into a BAML DynamicSchema (supports multiple tools, property_name mapping, and docstring parsing) âœ…
- [x] Implement `bind_tools()` using existing `convert_to_baml_tool()` âœ… COMPLETED AND TESTED
- [x] Implement `_convert_tool()` for tool format conversion âœ… COMPLETED

### Testing & Validation âœ… 80% Complete
- [x] End-to-end testing with real BAML functions âœ… COMPLETED
- [x] Performance benchmarking (189-401ms per call) âœ… COMPLETED
- [x] Async streaming functionality âœ… COMPLETED
- [x] Tool binding and conversion âœ… COMPLETED
- [ ] LangGraph workflow integration testing âŒ
- [ ] Comprehensive error handling validation âŒ

### Advanced Features âŒ 0% Complete
- [x] Streaming responses implementation - **Already implemented in `_astream()`** âœ…
- [ ] Multiple tool selection patterns âŒ
- [ ] Advanced configuration options âŒ
- [ ] Enhanced error handling âŒ

## ðŸ“‹ Implementation Notes

### Async-Only Architecture
Due to BAML's limitation of supporting only async OR sync at a time, this implementation uses an async-only approach:

- **Synchronous Methods**: `_generate()` and `_stream()` raise `NotImplementedError` with clear guidance to use async alternatives
- **BAML Configuration**: `generators.baml` set to `default_client_mode async` to align with implementation
- **Performance**: Excellent response times (189-401ms per call) with efficient token usage (45-56 output tokens)

### Current Status: 90% Complete
The core BAML integration is fully functional with:
- âœ… Complete async implementation (`_agenerate`, `_astream`, `bind_tools`)
- âœ… Full tool conversion system (`convert_to_baml_tool`)
- âœ… End-to-end testing with real BAML functions
- âœ… Transparent BAML logging and debugging
- âœ… Multi-step reasoning and automatic tool selection

### Usage Pattern
```python
# Async usage (recommended)
result = await chat_baml.agenerate(messages)
async for chunk in chat_baml.astream(messages):
    print(chunk)

# Tool binding
chat_baml_with_tools = chat_baml.bind_tools([AddTool, MultiplyTool])
result = await chat_baml_with_tools.ainvoke(messages)
```
---

## Custom LangChain Chat Model with BAML Integration

- **Fork of**: [tranngocphu/custom_langchain_chat_model](https://github.com/tranngocphu/custom_langchain_chat_model)
- **Enhancement**: BAML (BoundaryML) integration for structured data extraction and function calling
- **Status**: Active Development âš ï¸

### Key Enhancements from Original

- **BAML Integration**: LangChain-compatible wrapper with BAML function access
- **Automatic Tool Conversion**: Pydantic models â†’ BAML schemas via `convert_to_baml_tool`
- **Enhanced Tool Support**: Support for both Pydantic BaseModel and `@tool` decorated functions
- **Dynamic Configuration**: Environment-based setup with multiple provider support

## Quick Start

### Installation

```bash
git clone https://github.com/thanhhuynhk17/ChatBaml.git
uv venv
source .venv/bin/activate
uv sync
```

### BAML Code Generation

```bash
# The next commmand will auto-generate the baml_client directory, which will have auto-generated python code to call your BAML functions.
uv run baml-cli generate
```

### Environment Setup

Create a `.env` file with your API credentials:

```bash
OPENAI_MODEL_NAME="qwen3-vl"
OPENAI_BASE_URL="http://localhost:8000/v1"
OPENAI_API_KEY="sk_some_dummy_text"

# Set the default role in your .env when using an OpenAI-compatible server hosted by vLLM
DEFAULT_ROLE="user"
```

## convert_to_baml_tool Usage

The `convert_to_baml_tool` function in `custom_langchain_model/helpers/parse_json_schema.py` automatically converts tools to BAML schemas.

### Supported Tool Types

#### 1. Pydantic BaseModel (Recommended)

```python
from pydantic import BaseModel, Field

class AddTool(BaseModel):
    """Use this tool when you need to add two integers."""
    a: int = Field(..., description="First integer to add")
    b: int = Field(..., description="Second integer to add")

class MultiplyTool(BaseModel):
    """Use this tool when you need to multiply two integers."""
    a: int = Field(..., description="First integer to multiply")
    b: int = Field(..., description="Second integer to multiply")
```

#### 2. LangChain @tool Decorated Functions

**Important**: `@tool` functions must use `@tool(parse_docstring=True)` and have well-documented docstrings with Args and Returns sections.

```python
from langchain.tools import tool

@tool(parse_docstring=True)
def count_words(text: str) -> int:
    """Count the number of words in the provided text.
    
    Counts words by splitting on whitespace. Returns the total number of words.
    
    Args:
        text (str): Input text to count words from.
    
    Returns:
        int: Number of words in the input text.
    """
    if text is None:
        return 0
    words = re.findall(r"\S+", text.strip())
    return len(words)
```

### Usage Example

```python
from custom_langchain_model.helpers.parse_json_schema import convert_to_baml_tool
from baml_client import b
from baml_client.types import BamlState, BaseMessage

# Convert tools to BAML schema
tb = convert_to_baml_tool(
    tools=[AddTool, MultiplyTool, count_words],
    property_name="structure_output",
    is_multiple_tools=True
)

# Create BAML state
baml_state = BamlState(
    messages=[
        BaseMessage(
            role='system',
            content="You are an agent that can help with many tasks. Follow instructions and provide concise, useful responses."
        ),
        BaseMessage(
            role='user',
            content='I have a pen, i have 3 others, boom, what result would be when i combined them'
        )
    ]
)

# Call BAML function
response = await b.ChooseTool(baml_state, {"tb": tb}) # b is async client
print(f"Selected tool: {response.structure_output}") # response with property_name="structure_output"
```

**Real Test Output**:
```
Selected tool: [{'action': 'tool_AddTool', 'a': 1, 'b': 3}]
```

### BAML Logging (Transparent)

The BAML integration provides transparent logging that shows exactly what's happening under the hood:

```bash
(ChatBaml) vllm_user@idc-2-97:~/git_repos/ChatBaml$ PYTHONPATH=. uv run python custom_langchain_model/helpers/parse_json_schema.py
2026-01-25T12:33:40.428 [BAML INFO] Function ChooseTool:
    Client: ChatBaml (qwen3-vl) - 319ms. StopReason: stop. Tokens(in/out): 289/42
    ---PROMPT---
    system: Answer in JSON using this schema:
    {
      structure_output: [
        {
          // Use this tool when you found that you need to quickly add two integers.
          action: "tool_AddTool",
          // First integer to add
          a: int,
          // Second integer to add
          b: int,
        } or {
          // Use this tool when you found that you need to quickly multiply two integers.
          action: "tool_MultiplyTool",
          // First integer to multiply
          a: int,
          // Second integer to multiply
          b: int,
        } or {
          // Count the number of words in the provided text. Counts words by splitting on whitespace. Returns the total number of words.
          action: "tool_count_words",
          // Input text to count words from.
          text: string,
        } or {
          // Use this tool when you want to send a natural language response shown to the user. Write naturally, kindly, concisely when possible.
          action: "reply_to_user",
          message: {
            role: "assistant",
            content: string,
          },
        }
      ],
    }
              
    You are an agent that can help with many tasks. Follow instructions and provide concise, useful responses.
    user: I have a pen, i have 3 others, boom, what result would be when i combined them
    
    ---LLM REPLY---
    ```json
    {
      "structure_output": [
        {
          "action": "tool_AddTool",
          "a": 1,
          "b": 3
        }
      ]
    }
    ```
    ---Parsed Response (class DynamicSchema)---
    {
      "structure_output": [
        {
          "action": "tool_AddTool",
          "a": 1,
          "b": 3
        }
      ]
    }
response:
structure_output=[{'action': 'tool_AddTool', 'a': 1, 'b': 3}]
```

This logging shows:
- **Function Call**: Which BAML function was called (`ChooseTool`)
- **Client Info**: Model used (`qwen3-vl`) with performance metrics
- **Complete Prompt**: The exact JSON schema and user input sent to the LLM
- **LLM Response**: The raw JSON response from the model
- **Parsed Output**: How BAML parses the response back to the application


## Current Development Status

### âœ… Working Components

- **Tool Conversion System**: `convert_to_baml_tool` successfully converts Pydantic models and LangChain tools to BAML schemas
- **BAML Integration**: Basic BAML function calls work correctly
- **Environment Configuration**: Environment-based setup with secure credential management
- **Testing Framework**: Comprehensive unit tests for tool conversion

### âš ï¸ In Progress

- **ChatBaml Validation**: `chat_baml.py` integration with BAML functions - `_chat_completion_request()` âœ… COMPLETED AND TESTED
- **Advanced Features**: Streaming responses, multiple tool selection patterns
- **Performance Optimization**: Benchmarking and optimization for production use

### ðŸ“‹ Recent Test Results

The `convert_to_baml_tool` functionality has been successfully tested:

```
âœ… Tool conversion working: Pydantic models â†’ BAML schemas
âœ… BAML function calls successful: ChooseTool returned valid tool selection
âœ… Schema parsing correct: Response properly parsed to DynamicSchema
âœ… Integration functional: End-to-end tool selection and execution
```

## ðŸš€ Real-World Usage Example

Here's a complete example showing how the BAML + LangChain integration works in practice:

### Complete Conversation Flow

**User Input:**
```
What's the sum of 54 and 30, and the product of 64 and 16?
```

**BAML Tool Selection Process:**

#### 1. First Tool Selection (Addition)
```json
{
  "selected_tool": {
    "name": "add",
    "arguments": {
      "a": 54,
      "b": 30
    }
  }
}
```
- **Tool Execution**: `add(54, 30)` â†’ `84`
- **Performance**: 342ms, 169 input tokens, 45 output tokens

#### 2. Second Tool Selection (Multiplication)
```json
{
  "selected_tool": {
    "name": "multiply",
    "arguments": {
      "x": 64,
      "y": 16
    }
  }
}
```
- **Tool Execution**: `multiply(64, 16)` â†’ `1024`
- **Performance**: 189ms, 208 input tokens, 24 output tokens

#### 3. Final Response Generation
```json
{
  "selected_tool": {
    "name": "reply_to_user",
    "arguments": {
      "role": "assistant",
      "content": "The sum of 54 and 30 is 84, and the product of 64 and 16 is 1024."
    }
  }
}
```
- **Performance**: 401ms, 249 input tokens, 56 output tokens

**Final AI Response:**
```
The sum of 54 and 30 is 84, and the product of 64 and 16 is 1024.
```

### What This Demonstrates

1. **Multi-Step Reasoning**: BAML correctly breaks down complex questions into multiple tool calls
2. **Automatic Tool Selection**: The system automatically chooses the right tools (`add`, `multiply`, `reply_to_user`)
3. **Context Management**: Maintains conversation context across multiple tool calls
4. **Performance**: Excellent response times (189-401ms per call)
5. **Token Efficiency**: Efficient token usage (45-56 output tokens per call)

### BAML Logging Output

The integration provides transparent logging showing exactly what happens:

```bash
2026-01-26T15:56:39.102 [BAML INFO] Function ChooseTool:
    Client: OpenAIGeneric (qwen3-vl) - 347ms. StopReason: stop. Tokens(in/out): 167/44
    ---PROMPT---
    system: You are a helpful assistant.

    Answer in JSON using this schema:
    {
      selected_tool: {
        // Add two integers.
        name: "add",
        arguments: {
          a: int,
          b: int,
        },
      } or {
        // Multiply two integers.
        name: "multiply",
        arguments: {
          x: int,
          y: int,
        },
      } or {
        // Use this tool when you want to send a natural language response shown to the user.
        name: "reply_to_user",
        arguments: {
          role: "assistant",
          content: string,
        },
      },
    }
    user: What's the sum of 1 and 91, and the product of 27 and 9?

    ---LLM REPLY---
    ```json
    {
      "selected_tool": {
        "name": "add",
        "arguments": {
          "a": 1,
          "b": 91
        }
      }
    }
    ```
    ---Parsed Response (class DynamicSchema)---
    {
      "selected_tool": {
        "name": "add",
        "arguments": {
          "a": 1,
          "b": 91
        }
      }
    }
2026-01-26T15:56:39.357 [BAML INFO] Function ChooseTool:
    Client: OpenAIGeneric (qwen3-vl) - 182ms. StopReason: stop. Tokens(in/out): 205/23
    ---PROMPT---
    system: You are a helpful assistant.

    Answer in JSON using this schema:
    {
      selected_tool: {
        // Add two integers.
        name: "add",
        arguments: {
          a: int,
          b: int,
        },
      } or {
        // Multiply two integers.
        name: "multiply",
        arguments: {
          x: int,
          y: int,
        },
      } or {
        // Use this tool when you want to send a natural language response shown to the user.
        name: "reply_to_user",
        arguments: {
          role: "assistant",
          content: string,
        },
      },
    }
    user: What's the sum of 1 and 91, and the product of 27 and 9?
    assistant: {"name": "add", "arguments": {"a": 1, "b": 91}}
    tool: 92

    ---LLM REPLY---
    {"name": "multiply", "arguments": {"x": 27, "y": 9}}
    ---Parsed Response (class DynamicSchema)---
    {
      "selected_tool": {
        "name": "multiply",
        "arguments": {
          "x": 27,
          "y": 9
        }
      }
    }
2026-01-26T15:56:39.811 [BAML INFO] Function ChooseTool:
    Client: OpenAIGeneric (qwen3-vl) - 379ms. StopReason: stop. Tokens(in/out): 244/53
    ---PROMPT---
    system: You are a helpful assistant.

    Answer in JSON using this schema:
    {
      selected_tool: {
        // Add two integers.
        name: "add",
        arguments: {
          a: int,
          b: int,
        },
      } or {
        // Multiply two integers.
        name: "multiply",
        arguments: {
          x: int,
          y: int,
        },
      } or {
        // Use this tool when you want to send a natural language response shown to the user.
        name: "reply_to_user",
        arguments: {
          role: "assistant",
          content: string,
        },
      },
    }
    user: What's the sum of 1 and 91, and the product of 27 and 9?
    assistant: {"name": "add", "arguments": {"a": 1, "b": 91}}
    tool: 92
    assistant: {"name": "multiply", "arguments": {"x": 27, "y": 9}}
    tool: 243

    ---LLM REPLY---
    {"name": "reply_to_user", "arguments": {"role": "assistant", "content": "The sum of 1 and 91 is 92, and the product of 27 and 9 is 243."}}
    ---Parsed Response (class DynamicSchema)---
    {
      "selected_tool": {
        "name": "reply_to_user",
        "arguments": {
          "role": "assistant",
          "content": "The sum of 1 and 91 is 92, and the product of 27 and 9 is 243."
        }
      }
    }
```

This logging shows:
- **Function Call**: Which BAML function was called (`ChooseTool`)
- **Client Info**: Model used (`qwen3-vl`) with performance metrics
- **Complete Prompt**: The exact JSON schema and user input sent to the LLM
- **LLM Response**: The raw JSON response from the model
- **Parsed Output**: How BAML parses the response back to the application

## Requirements

- Python >= 3.13
- Key dependencies:
  - `baml-py>=0.218.0`
  - `langchain>=1.0.0`
  - `langgraph>=1.0.0`
  - `pydantic>=2.12.3`
  - `pydantic-settings>=2.11.0`

## Development

### Running Tests

```bash
# Run all tests
pytest

# Run specific test file
pytest test/unit/test_convert_to_baml_tool.py

# Run with coverage
pytest --cov=custom_langchain_model
```

### Testing convert_to_baml_tool

```bash
# Run the conversion test script
PYTHONPATH=. uv run python custom_langchain_model/helpers/parse_json_schema.py
```