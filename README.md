# Overview

This project extends a Custom LangChain Chat Model to integrate with BAML, creating a bridge between LangChain's powerful LLM orchestration capabilities and BAML's structured data extraction and function calling features.

## ðŸš€ BAML Model Integration TODO

### Core Implementation
- [x] Create `BamlChatModel` extending `BaseChatModel`
- [x] Implement `_chat_completion_request()` for BAML API calls âœ… COMPLETED
- [ ] Implement `_agenerate()` method following tutorial pattern
- [ ] Implement `_generate()` method (sync fallback)
- [ ] Implement `_prepare_messages()` for message conversion

### Tool Integration
- [x] Implement `convert_to_baml_tool()` â€” converts Pydantic BaseModel and LangChain `@tool(parse_docstring=True)` functions into a BAML DynamicSchema (supports multiple tools, property_name mapping, and docstring parsing)
- [ ] Implement `bind_tools()` using existing `convert_to_baml_tool()`
- [ ] Implement `_convert_tool()` for tool format conversion

- [ ] ...
---

## Custom LangChain Chat Model with BAML Integration

- **Fork of**: [tranngocphu/custom_langchain_chat_model](https://github.com/tranngocphu/custom_langchain_chat_model)
- **Enhancement**: BAML (BoundaryML) integration for structured data extraction and function calling
- **Status**: Active Development âš ï¸

### Key Enhancements from Original

- **BAML Integration**: LangChain-compatible wrapper with BAML function access
- **Automatic Tool Conversion**: Pydantic models â†’ BAML schemas via `convert_to_baml_tool`
- **Enhanced Tool Support**: Support for both Pydantic BaseModel and `@tool` decorated functions
- **Dynamic Configuration**: Environment-based setup with multiple provider support

## Quick Start

### Installation

```bash
git clone https://github.com/thanhhuynhk17/ChatBaml.git
uv venv
source .venv/bin/activate
uv sync
```

### BAML Code Generation

```bash
# The next commmand will auto-generate the baml_client directory, which will have auto-generated python code to call your BAML functions.
uv run baml-cli generate
```

### Environment Setup

Create a `.env` file with your API credentials:

```bash
OPENAI_MODEL_NAME="qwen3-vl"
OPENAI_BASE_URL="http://localhost:8000/v1"
OPENAI_API_KEY="sk_some_dummy_text"

# Set the default role in your .env when using an OpenAI-compatible server hosted by vLLM
DEFAULT_ROLE="user"
```

## convert_to_baml_tool Usage

The `convert_to_baml_tool` function in `custom_langchain_model/helpers/parse_json_schema.py` automatically converts tools to BAML schemas.

### Supported Tool Types

#### 1. Pydantic BaseModel (Recommended)

```python
from pydantic import BaseModel, Field

class AddTool(BaseModel):
    """Use this tool when you need to add two integers."""
    a: int = Field(..., description="First integer to add")
    b: int = Field(..., description="Second integer to add")

class MultiplyTool(BaseModel):
    """Use this tool when you need to multiply two integers."""
    a: int = Field(..., description="First integer to multiply")
    b: int = Field(..., description="Second integer to multiply")
```

#### 2. LangChain @tool Decorated Functions

**Important**: `@tool` functions must use `@tool(parse_docstring=True)` and have well-documented docstrings with Args and Returns sections.

```python
from langchain.tools import tool

@tool(parse_docstring=True)
def count_words(text: str) -> int:
    """Count the number of words in the provided text.
    
    Counts words by splitting on whitespace. Returns the total number of words.
    
    Args:
        text (str): Input text to count words from.
    
    Returns:
        int: Number of words in the input text.
    """
    if text is None:
        return 0
    words = re.findall(r"\S+", text.strip())
    return len(words)
```

### Usage Example

```python
from custom_langchain_model.helpers.parse_json_schema import convert_to_baml_tool
from baml_client import b
from baml_client.types import BamlState, BaseMessage

# Convert tools to BAML schema
tb = convert_to_baml_tool(
    tools=[AddTool, MultiplyTool, count_words],
    property_name="structure_output",
    is_multiple_tools=True
)

# Create BAML state
baml_state = BamlState(
    messages=[
        BaseMessage(
            role='system',
            content="You are an agent that can help with many tasks. Follow instructions and provide concise, useful responses."
        ),
        BaseMessage(
            role='user',
            content='I have a pen, i have 3 others, boom, what result would be when i combined them'
        )
    ]
)

# Call BAML function
response = await b.ChooseTool(baml_state, {"tb": tb}) # b is async client
print(f"Selected tool: {response.structure_output}") # response with property_name="structure_output"
```

**Real Test Output**:
```
Selected tool: [{'action': 'tool_AddTool', 'a': 1, 'b': 3}]
```

### BAML Logging (Transparent)

The BAML integration provides transparent logging that shows exactly what's happening under the hood:

```bash
(ChatBaml) vllm_user@idc-2-97:~/git_repos/ChatBaml$ PYTHONPATH=. uv run python custom_langchain_model/helpers/parse_json_schema.py
2026-01-25T12:33:40.428 [BAML INFO] Function ChooseTool:
    Client: ChatBaml (qwen3-vl) - 319ms. StopReason: stop. Tokens(in/out): 289/42
    ---PROMPT---
    system: Answer in JSON using this schema:
    {
      structure_output: [
        {
          // Use this tool when you found that you need to quickly add two integers.
          action: "tool_AddTool",
          // First integer to add
          a: int,
          // Second integer to add
          b: int,
        } or {
          // Use this tool when you found that you need to quickly multiply two integers.
          action: "tool_MultiplyTool",
          // First integer to multiply
          a: int,
          // Second integer to multiply
          b: int,
        } or {
          // Count the number of words in the provided text. Counts words by splitting on whitespace. Returns the total number of words.
          action: "tool_count_words",
          // Input text to count words from.
          text: string,
        } or {
          // Use this tool when you want to send a natural language response shown to the user. Write naturally, kindly, concisely when possible.
          action: "reply_to_user",
          message: {
            role: "assistant",
            content: string,
          },
        }
      ],
    }
              
    You are an agent that can help with many tasks. Follow instructions and provide concise, useful responses.
    user: I have a pen, i have 3 others, boom, what result would be when i combined them
    
    ---LLM REPLY---
    ```json
    {
      "structure_output": [
        {
          "action": "tool_AddTool",
          "a": 1,
          "b": 3
        }
      ]
    }
    ```
    ---Parsed Response (class DynamicSchema)---
    {
      "structure_output": [
        {
          "action": "tool_AddTool",
          "a": 1,
          "b": 3
        }
      ]
    }
response:
structure_output=[{'action': 'tool_AddTool', 'a': 1, 'b': 3}]
```

This logging shows:
- **Function Call**: Which BAML function was called (`ChooseTool`)
- **Client Info**: Model used (`qwen3-vl`) with performance metrics
- **Complete Prompt**: The exact JSON schema and user input sent to the LLM
- **LLM Response**: The raw JSON response from the model
- **Parsed Output**: How BAML parses the response back to the application


## Current Development Status

### âœ… Working Components

- **Tool Conversion System**: `convert_to_baml_tool` successfully converts Pydantic models and LangChain tools to BAML schemas
- **BAML Integration**: Basic BAML function calls work correctly
- **Environment Configuration**: Environment-based setup with secure credential management
- **Testing Framework**: Comprehensive unit tests for tool conversion

### âš ï¸ In Progress

- **ChatBaml Validation**: `chat_baml.py` integration with BAML functions - `_chat_completion_request()` âœ… COMPLETED AND TESTED
- **Advanced Features**: Streaming responses, multiple tool selection patterns
- **Performance Optimization**: Benchmarking and optimization for production use

### ðŸ“‹ Recent Test Results

The `convert_to_baml_tool` functionality has been successfully tested:

```
âœ… Tool conversion working: Pydantic models â†’ BAML schemas
âœ… BAML function calls successful: ChooseTool returned valid tool selection
âœ… Schema parsing correct: Response properly parsed to DynamicSchema
âœ… Integration functional: End-to-end tool selection and execution
```

## Requirements

- Python >= 3.13
- Key dependencies:
  - `baml-py>=0.218.0`
  - `langchain>=1.0.0`
  - `langgraph>=1.0.0`
  - `pydantic>=2.12.3`
  - `pydantic-settings>=2.11.0`

## Development

### Running Tests

```bash
# Run all tests
pytest

# Run specific test file
pytest test/unit/test_convert_to_baml_tool.py

# Run with coverage
pytest --cov=custom_langchain_model
```

### Testing convert_to_baml_tool

```bash
# Run the conversion test script
PYTHONPATH=. uv run python custom_langchain_model/helpers/parse_json_schema.py
```